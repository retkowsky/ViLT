# Visual Question Answering with ViLT

ViLT = Vision-and-Language Pre-training

The ViLT model was proposed in ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision by Wonjae Kim, Bokyung Son, Ildoo Kim.
ViLT incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for Vision-and-Language Pre-training (VLP).

https://huggingface.co/docs/transformers/model_doc/vilt

## Notebook
<a href="https://github.com/retkowsky/ViLT/blob/main/Visual%20Question%20Answering%20with%20ViLT.ipynb"> Python notebook demo <a>

## Demo
<img src="ViLT.gif">

20-Jan-2023 Serge Retkowsky | serge.retkowsky@microsoft.com | https://www.linkedin.com/in/serger/
